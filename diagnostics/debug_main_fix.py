# Version DEBUG avec chargement FAISS s√©curis√©
import os
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()

# Chargement FAISS avec gestion d'erreur
DB_FAISS_PATH = "Logs/vector_index_chatgpt"
embedding = OpenAIEmbeddings()

print("üîÑ Tentative de chargement de l'index FAISS...")

try:
    # M√©thode 1 : Chargement normal
    vectorstore = FAISS.load_local(
        DB_FAISS_PATH, 
        embedding, 
        allow_dangerous_deserialization=True
    )
    print("‚úÖ Index FAISS charg√© avec succ√®s (m√©thode normale)")
    
except Exception as e1:
    print(f"‚ùå M√©thode 1 √©chou√©e : {e1}")
    try:
        # M√©thode 2 : Chargement alternatif
        import faiss
        import pickle
        
        # Chargement manuel des composants
        index_path = os.path.join(DB_FAISS_PATH, "index.faiss")
        pkl_path = os.path.join(DB_FAISS_PATH, "index.pkl")
        
        # Charger l'index FAISS
        index = faiss.read_index(index_path)
        print(f"‚úÖ Index FAISS brut charg√© : {index.ntotal} vecteurs")
        
        # Charger les m√©tadonn√©es
        with open(pkl_path, 'rb') as f:
            store_data = pickle.load(f)
        print(f"‚úÖ M√©tadonn√©es charg√©es : {len(store_data)} entr√©es")
        
        # Cr√©er le vectorstore manuellement
        vectorstore = FAISS(
            embedding_function=embedding,
            index=index,
            docstore=store_data.get('docstore', {}),
            index_to_docstore_id=store_data.get('index_to_docstore_id', {})
        )
        print("‚úÖ Vectorstore reconstruit manuellement")
        
    except Exception as e2:
        print(f"‚ùå Toutes les m√©thodes ont √©chou√© : {e2}")
        print("üîç Contenu du dossier :")
        for file in os.listdir(DB_FAISS_PATH):
            file_path = os.path.join(DB_FAISS_PATH, file)
            size = os.path.getsize(file_path) / (1024*1024)  # MB
            print(f"  - {file}: {size:.1f} MB")
        exit(1)

# Test du retriever
print("\nüîç TEST DU RETRIEVER :")
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

test_query = "douleur main droite"
try:
    docs = retriever.get_relevant_documents(test_query)
    print(f"üìÑ DOCUMENTS TROUV√âS ({len(docs)}) :")
    for i, doc in enumerate(docs):
        print(f"\n--- Document {i+1} ---")
        print(f"Contenu: {doc.page_content[:200]}...")
        if hasattr(doc, 'metadata'):
            print(f"Metadata: {doc.metadata}")
except Exception as e:
    print(f"‚ùå Erreur lors de la recherche : {e}")
    exit(1)

# Configuration du prompt
debug_prompt_template = """Utilise les informations suivantes pour r√©pondre √† la question de l'utilisateur.
Si tu ne connais pas la r√©ponse, dis-le clairement. Ne tente pas d'inventer une r√©ponse.

Contexte : {context}
---
Question : {question}
R√©ponse : """

prompt = PromptTemplate(
    template=debug_prompt_template,
    input_variables=["context", "question"]
)

# Mod√®le LLM
llm = ChatOpenAI(model_name="gpt-4", temperature=0)

# Cha√Æne QA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=False
)

print("\n" + "="*50)
print("‚úÖ RAG en mode DEBUG - Tapez 'exit' pour quitter")
print("="*50)

while True:
    query = input("\nVous: ")
    if query.lower() in ["exit", "quit", "q"]:
        print("üëã Fin du RAG.")
        break
    
    try:
        result = qa_chain.run(query)
        print(f"\nüìù R√âPONSE : {result}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration : {e}")
