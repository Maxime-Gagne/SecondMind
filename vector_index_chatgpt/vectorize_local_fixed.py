# -*- coding: utf-8 -*-
"""
Vectorisation LOCALE avec HuggingFace Embeddings
Version corrig√©e avec chemins absolus et am√©liorations
"""
import os
import sys
import json
import pickle
import numpy as np
from datetime import datetime
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings

def nettoyer_ligne(texte):
    """Nettoie et standardise une ligne de texte"""
    return texte.strip().replace("\n", " ").replace("\r", "").replace("  ", " ").strip()

def extraire_role_et_contenu(ligne):
    """Extrait le r√¥le et le contenu d'une ligne de conversation"""
    ligne_nettoyee = nettoyer_ligne(ligne)
    if not ligne_nettoyee:
        return None, None
    
    # D√©tection du format "role|contenu" ou similaire
    if "|" in ligne_nettoyee:
        parts = ligne_nettoyee.split("|", 1)
        role = parts[0].strip().lower()
        contenu = parts[1].strip()
    elif ligne_nettoyee.lower().startswith(("user:", "assistant:", "human:", "ai:")):
        parts = ligne_nettoyee.split(":", 1)
        role = parts[0].strip().lower()
        contenu = parts[1].strip() if len(parts) > 1 else ""
    else:
        role = "unknown"
        contenu = ligne_nettoyee
    
    return role, contenu

def main():
    print("üöÄ D√âMARRAGE DE LA VECTORISATION LOCALE (HuggingFace)")
    print("=" * 65)
    
    # === CHEMINS ABSOLUS FIXES ===
    BASE_DIR = r"C:\Users\rag_personnel"
    DATA_PATH = os.path.join(BASE_DIR, "Logs", "conversations_extraites.txt")
    DB_FAISS_PATH = os.path.join(BASE_DIR, "Logs", "vector_index_chatgpt")
    
    print(f"üìÅ Fichier source : {DATA_PATH}")
    print(f"üìÅ Dossier index : {DB_FAISS_PATH}")
    
    # === V√âRIFICATIONS PR√âLIMINAIRES ===
    if not os.path.exists(DATA_PATH):
        print(f"‚ùå ERREUR : Fichier source introuvable : {DATA_PATH}")
        input("Appuyez sur Entr√©e pour fermer...")
        sys.exit(1)
    
    # Cr√©er le dossier de destination si n√©cessaire
    os.makedirs(DB_FAISS_PATH, exist_ok=True)
    
    # === LECTURE ET TRAITEMENT DES DONN√âES ===
    print("\nüìñ Lecture du fichier source...")
    try:
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            all_lines = f.readlines()
    except Exception as e:
        print(f"‚ùå ERREUR lors de la lecture : {e}")
        input("Appuyez sur Entr√©e pour fermer...")
        sys.exit(1)
    
    print(f"‚úÖ {len(all_lines)} lignes lues")
    
    # === CR√âATION DES DOCUMENTS ===
    print("\nüîÑ Traitement des documents...")
    docs = []
    stats = {"user": 0, "assistant": 0, "unknown": 0, "empty": 0}
    
    for i, line in enumerate(all_lines):
        role, contenu = extraire_role_et_contenu(line)
        
        if not contenu:
            stats["empty"] += 1
            continue
        
        # Mise √† jour des statistiques
        if role in stats:
            stats[role] += 1
        else:
            stats["unknown"] += 1
        
        # Cr√©ation du document
        metadata = {
            "source": "conversations_extraites.txt",
            "ligne": i + 1,
            "role": role,
            "longueur": len(contenu),
            "timestamp": datetime.now().isoformat()
        }
        
        doc = Document(page_content=contenu, metadata=metadata)
        docs.append(doc)
    
    print(f"‚úÖ {len(docs)} documents cr√©√©s")
    print(f"üìä Statistiques : {stats}")
    
    if not docs:
        print("‚ùå ERREUR : Aucun document valide trouv√©")
        input("Appuyez sur Entr√©e pour fermer...")
        sys.exit(1)
    
    # === INITIALISATION DES EMBEDDINGS LOCAUX ===
    print("\nüß† Initialisation des embeddings locaux...")
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    
    try:
        # M√©thode 1 : Utiliser HuggingFaceEmbeddings (recommand√© pour LangChain)
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},  # Utiliser 'cuda' si GPU disponible
            encode_kwargs={'normalize_embeddings': True}
        )
        print("‚úÖ Embeddings HuggingFace initialis√©s")
        
        # Test rapide des embeddings
        test_embedding = embeddings.embed_query("test")
        print(f"‚úÖ Test d'embedding r√©ussi (dimension: {len(test_embedding)})")
        
    except Exception as e:
        print(f"‚ùå ERREUR lors de l'initialisation des embeddings : {e}")
        print("üí° Essayez d'installer : pip install sentence-transformers")
        input("Appuyez sur Entr√©e pour fermer...")
        sys.exit(1)
    
    # === VECTORISATION ===
    print("\nüîÑ Vectorisation en cours...")
    try:
        # Traitement par batch pour g√©rer la m√©moire
        batch_size = 50  # Plus petit pour le local
        
        if len(docs) > batch_size:
            print(f"üì¶ Traitement par batch de {batch_size} documents")
            
            # Premier batch pour initialiser l'index
            first_batch = docs[:batch_size]
            index = FAISS.from_documents(first_batch, embeddings)
            print(f"‚úÖ Premier batch trait√© : {len(first_batch)} documents")
            
            # Batches suivants
            for i in range(batch_size, len(docs), batch_size):
                batch = docs[i:i+batch_size]
                batch_index = FAISS.from_documents(batch, embeddings)
                index.merge_from(batch_index)
                print(f"‚úÖ Batch {i//batch_size + 1} trait√© : {len(batch)} documents")
                
                # Affichage du progr√®s
                progress = min(i + batch_size, len(docs))
                percentage = (progress / len(docs)) * 100
                print(f"üìä Progr√®s : {progress}/{len(docs)} ({percentage:.1f}%)")
        else:
            index = FAISS.from_documents(docs, embeddings)
        
        print("‚úÖ Vectorisation termin√©e")
        
    except Exception as e:
        print(f"‚ùå ERREUR lors de la vectorisation : {e}")
        input("Appuyez sur Entr√©e pour fermer...")
        sys.exit(1)
    
    # === SAUVEGARDE ===
    print("\nüíæ Sauvegarde de l'index...")
    try:
        index.save_local(DB_FAISS_PATH)
        print("‚úÖ Index FAISS sauvegard√©")
        
        # Sauvegarde des m√©tadonn√©es suppl√©mentaires
        metadata_file = os.path.join(DB_FAISS_PATH, "metadata.json")
        metadata_info = {
            "created_at": datetime.now().isoformat(),
            "source_file": DATA_PATH,
            "total_documents": len(docs),
            "embedding_model": model_name,
            "embedding_type": "local_huggingface",
            "stats": stats,
            "version": "2.0"
        }
        
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata_info, f, indent=2, ensure_ascii=False)
        
        print("‚úÖ M√©tadonn√©es sauvegard√©es")
        
        # Sauvegarde des m√©tadonn√©es pour compatibilit√© avec les anciens scripts
        metadatas_list = []
        for doc in docs:
            metadatas_list.append({
                "ligne_originale": doc.metadata["ligne"],
                "role": doc.metadata["role"],
                "texte_complet": doc.page_content,
                "longueur": doc.metadata["longueur"]
            })
        
        with open(os.path.join(DB_FAISS_PATH, "index.pkl"), "wb") as f:
            pickle.dump(metadatas_list, f)
        
        print("‚úÖ M√©tadonn√©es de compatibilit√© sauvegard√©es")
        
    except Exception as e:
        print(f"‚ùå ERREUR lors de la sauvegarde : {e}")
        input("Appuyez sur Entr√©e pour fermer...")
        sys.exit(1)
    
    # === CR√âATION DU DIAGNOSTIC ===
    diagnostic_file = os.path.join(DB_FAISS_PATH, "diagnostic.txt")
    try:
        with open(diagnostic_file, "w", encoding="utf-8") as f:
            f.write(f"=== DIAGNOSTIC VECTORISATION LOCALE ===\n")
            f.write(f"Date de cr√©ation : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Fichier source : {DATA_PATH}\n")
            f.write(f"Mod√®le d'embedding : {model_name}\n")
            f.write(f"Nombre total de documents : {len(docs)}\n")
            f.write(f"Statistiques par r√¥le :\n")
            for role, count in stats.items():
                f.write(f"  - {role} : {count}\n")
            f.write(f"\nIndex sauvegard√© dans : {DB_FAISS_PATH}\n")
            f.write(f"‚úÖ Vectorisation r√©ussie\n")
        
        print("‚úÖ Diagnostic cr√©√©")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Avertissement : Impossible de cr√©er le diagnostic : {e}")
    
    # === TEST DE L'INDEX ===
    print("\nüß™ Test de l'index cr√©√©...")
    try:
        # Test de rechargement
        test_index = FAISS.load_local(
            DB_FAISS_PATH, 
            embeddings,
            allow_dangerous_deserialization=True
        )
        
        # Test de recherche
        retriever = test_index.as_retriever(search_kwargs={"k": 3})
        test_results = retriever.get_relevant_documents("conversation")
        
        print("‚úÖ Index test√© avec succ√®s")
        print(f"üîç Test de recherche : {len(test_results)} r√©sultats trouv√©s")
        
        if test_results:
            doc = test_results[0]
            print(f"üìÑ Exemple de r√©sultat :")
            print(f"   R√¥le : {doc.metadata.get('role', 'N/A')}")
            print(f"   Ligne : {doc.metadata.get('ligne', 'N/A')}")
            print(f"   Contenu : {doc.page_content[:100]}...")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Avertissement lors du test : {e}")
    
    # === R√âSUM√â FINAL ===
    print("\n" + "=" * 65)
    print("üéâ VECTORISATION LOCALE TERMIN√âE AVEC SUCC√àS !")
    print(f"‚úÖ {len(docs)} documents vectoris√©s")
    print(f"üìÅ Index sauvegard√© dans : {DB_FAISS_PATH}")
    print(f"üß† Mod√®le utilis√© : {model_name}")
    print("üí° Avantages du mod√®le local :")
    print("   - Aucun co√ªt d'API")
    print("   - Fonctionne hors ligne")
    print("   - Donn√©es priv√©es")
    print("üöÄ Vous pouvez maintenant lancer l'interface Gradio LOCAL")
    print("=" * 65)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Vectorisation interrompue par l'utilisateur")
    except Exception as e:
        print(f"\nüí• ERREUR CRITIQUE : {e}")
    finally:
        print("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour fermer...")
        input()